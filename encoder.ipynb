{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dexter/Desktop/projects/Voice-Cloning/hparam.py:11: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  for doc in docs:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "from utils import mfccs_and_spec\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from random import shuffle\n",
    "from hparam import hparam as hp\n",
    "from torch.functional import F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable, Function\n",
    "import torch.optim as optim\n",
    "from data_load import TripletSpeakerDataset\n",
    "from speech_embedder_net import SpeechEmbedder, GE2ELoss, get_centroids, get_cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dexter/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa \n",
    "import librosa.display\n",
    "import IPython\n",
    "import pickle \n",
    "import numpy as np\n",
    "import scipy \n",
    "import tensorflow as tf \n",
    "from sklearn.externals import joblib\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.saved_model import simple_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dexter/Desktop/Projects/Mini Project/Voice-Cloning/hparam.py:11: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  for doc in docs:\n"
     ]
    }
   ],
   "source": [
    "from utils import mfcc_for_accent\n",
    "from data_preprocess import get_spectrogram_tisv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement = True,\n",
    "                        device_count = {'CPU' : 6,\n",
    "                                       'GPU' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_accent_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(50, input_shape = (14976,), activation = 'relu',name='input'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(75, activation = 'tanh', name='h1'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(100,  activation = 'tanh',name = 'h2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(100,  activation = 'tanh', name = 'h3'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(100,  activation = 'tanh', name = 'h4'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(100,  activation = 'tanh', name = 'h5'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(100,  activation = 'tanh', name ='h6'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(3,  activation = 'softmax', name = 'output'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Mel spectrograms and MFCCs for Indians \n",
    "def get_accent_mfccs(path):\n",
    "    x, _ = librosa.load(path)\n",
    "    x, mf = mfcc_for_accent(x)\n",
    "    mf = mf.flatten()\n",
    "    #print(type(x))\n",
    "    # l.append(librosa.feature.mfcc(x, sr=f))\n",
    "    \n",
    "    dat = np.reshape(mf, (1,mf.shape[0]))\n",
    "    return x, dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SpeechEmbedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-01f53eb4893a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membedder_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpeechEmbedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membedder_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maccent_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_accent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccent_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accent_block_with_scottish.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SpeechEmbedder' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "embedder_net = SpeechEmbedder().to('cuda')\n",
    "embedder_net.load_state_dict(torch.load(hp.model.model_path))\n",
    "accent_net = make_accent_model()\n",
    "accent_net.load_weights('accent_block_with_scottish.h5')\n",
    "accent_net.pop()\n",
    "\n",
    "\n",
    "#wav, sr = librosa.load(file_name)\n",
    "\n",
    "def run_speaker_encoder(mfccs, embedder_net):\n",
    "    mel_db = torch.transpose(mfccs, 1,2)\n",
    "    #mel_db = torch.from_numpy(mel_db)\n",
    "    #print(mel_db.shape)\n",
    "    out = embedder_net(mel_db.to('cuda'))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_both_embs(path):\n",
    "        #for path in enumerate(train_dataset):\n",
    "        _, accent_mfccs = get_accent_mfccs(path)\n",
    "        speaker_mfccs = get_spectrogram_tisv(path)\n",
    "        #print(accent_mfccs.shape)\n",
    "        #print(speaker_mfccs.shape)\n",
    "        speaker_mfccs = Variable(torch.from_numpy(speaker_mfccs)).to('cuda')\n",
    "        speaker_emb = run_speaker_encoder(speaker_mfccs, embedder_net)\n",
    "        #print(speaker_emb.shape)\n",
    "        accent_emb = accent_net.predict(accent_mfccs)\n",
    "        #print(accent_emb.shape)\n",
    "        return speaker_emb.cpu().detach().numpy(), accent_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseDistance(Function):\n",
    "    def __init__(self, p):\n",
    "        super(PairwiseDistance, self).__init__()\n",
    "        self.norm = p\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        #print(\"above assert: \",x1, x2)\n",
    "        assert x1.size() == x2.size()\n",
    "        eps = 1e-4 / x1.size(1)\n",
    "        diff = torch.abs(x1 - x2)\n",
    "        out = torch.pow(diff, self.norm).sum(dim=1)\n",
    "        return torch.pow(out + eps, 1. / self.norm)\n",
    "\n",
    "class TripletLoss(Function):\n",
    "    \"\"\"\n",
    "    Triplet loss function.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.2):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.pdist = PairwiseDistance(2)  # norm 2\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        d_p = self.pdist.forward(torch.from_numpy(anchor), positive)\n",
    "        d_n = self.pdist.forward(torch.from_numpy(anchor), torch.from_numpy(negative))\n",
    "\n",
    "        dist_hinge = torch.clamp(self.margin + d_p - d_n, min=0.0)\n",
    "        loss = torch.mean(dist_hinge)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.inp1 = nn.Linear(in_features=256, out_features=100)\n",
    "        self.inp2 = nn.Linear(in_features=100, out_features=100)\n",
    "        self.b_n = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.drop_out = nn.Dropout(0.25)\n",
    "        self.l1 = nn.Linear(200, 128)\n",
    "        self.l2 = nn.Linear(128,128)\n",
    "        self.out_acc = nn.Linear(128, 100)\n",
    "        self.out_speak = nn.Linear(128,256)\n",
    "        \n",
    "    def forward(self, speaker_emb, acc_emb):\n",
    "        x1 = F.relu(self.inp1(speaker_emb))\n",
    "        \n",
    "        x2 = F.relu(self.inp2(x1))\n",
    "        #print(x1.shape, x2.shape, speaker_emb.shape, acc_emb.shape)\n",
    "        out = torch.cat((x1, x2), axis=1)\n",
    "        out = self.l1(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.relu(self.l2(out))\n",
    "        \n",
    "        mf_acc = self.out_acc(out)\n",
    "        mf_sp = self.out_speak(out)\n",
    "        return F.relu(mf_sp), F.relu(mf_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = hp.device\n",
    "    dataset = TripletSpeakerDataset()\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, drop_last=True)\n",
    "    ae = AutoEncoder().to('cuda')\n",
    "    writer = SummaryWriter('triplet_loss_logs')\n",
    "    optimizer = optim.Adam(ae.parameters())\n",
    "    optimizer.zero_grad()\n",
    "    ae.load_state_dict(torch.load('./new_checkpoints/ckpt_epoch_9200_batch_id_9200.pth'))\n",
    "    trip_loss = TripletLoss()\n",
    "    total_loss = 0\n",
    "    for epoch in range(20):\n",
    "        for batch_id, (anchor_utter_path, positive_utter_path, negative_utter_path, accent_negative_path, speaker_positive_id, speaker_negative_id, speaker_anchor_id, positive_accent, negative_accent) in tqdm(enumerate(data_loader), total = len(data_loader)):\n",
    "            anchor_utter_path, positive_utter_path, negative_utter_path = anchor_utter_path[0], positive_utter_path[0], negative_utter_path[0]\n",
    "            accent_negative_path, speaker_positive_id, speaker_negative_id = accent_negative_path[0], speaker_positive_id[0], speaker_negative_id[0]\n",
    "            speaker_anchor_id, positive_accent, negative_accent = speaker_anchor_id[0], positive_accent[0], negative_accent[0]\n",
    "            \n",
    "            try:\n",
    "                pre_positive_speaker_embs, pre_positive_accent_embs = get_both_embs(positive_utter_path)\n",
    "                pre_positive_speaker_embs, pre_positive_accent_embs = Variable(torch.from_numpy(pre_positive_speaker_embs)).to('cuda'), Variable(torch.from_numpy(pre_positive_accent_embs)).to('cuda')\n",
    "                anchor_utter_path\n",
    "                anchor_speaker_embs, anchor_accent_embs = get_both_embs(anchor_utter_path)\n",
    "                negative_speaker_embs, _ = get_both_embs(negative_utter_path)\n",
    "                _, negative_accent_embs = get_both_embs(accent_negative_path)\n",
    "            except ValueError:\n",
    "                print(\"ValueError\")\n",
    "                continue\n",
    "            except:\n",
    "                continue\n",
    "            positive_speaker_embs, positive_accent_embs = ae.forward(pre_positive_speaker_embs, pre_positive_accent_embs)\n",
    "            positive_speaker_embs, positive_accent_embs = positive_speaker_embs.to('cpu'), positive_accent_embs.to('cpu')\n",
    "            optimizer.zero_grad()\n",
    "            #print(anchor_accent_embs.shape)\n",
    "            accent_trip_loss = trip_loss.forward(anchor_accent_embs, positive_accent_embs, negative_accent_embs)\n",
    "            speaker_trip_loss = trip_loss.forward(anchor_speaker_embs, positive_speaker_embs, negative_speaker_embs)\n",
    "            criterion = accent_trip_loss + speaker_trip_loss\n",
    "            total_loss += criterion\n",
    "            \n",
    "            \n",
    "            \n",
    "            criterion.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('Loss', criterion, epoch*len(data_loader)+1)\n",
    "            \n",
    "            if (batch_id + 1) % 5 == 0:\n",
    "                mesg = \"{}\\tEpoch:{},Iteration:{}\\tLoss:{}\\t\\n\".format(time.ctime(), epoch+1,\n",
    "                        batch_id+1, criterion)\n",
    "                print(mesg)\n",
    "                if hp.train.log_file is not None:\n",
    "                    with open(hp.train.log_file,'a') as f:\n",
    "                        f.write(mesg)\n",
    "            if hp.train.checkpoint_dir is not None and (batch_id + 1) % 100 == 0:\n",
    "                ae.eval()\n",
    "                ckpt_model_filename = \"ckpt_epoch_\" + str(epoch+1) + \"_batch_id_\" + str(batch_id+1) + \".pth\"\n",
    "                ckpt_model_path = os.path.join(\"new_checkpoints/\", ckpt_model_filename)\n",
    "                state = {'epoch': epoch + 1, 'state_dict': ae.state_dict(), \n",
    "                         'optimizer': optimizer.state_dict(), }\n",
    "                torch.save(state, ckpt_model_path)\n",
    "                \n",
    "                ae.train()\n",
    "        ae.eval()\n",
    "        save_model_filename = \"final_epoch_\" + str(epoch + 1) + \"_batch_id_\" + str(batch_id + 1) + \".model\"\n",
    "        save_model_path = os.path.join(hp.train.checkpoint_dir, save_model_filename)\n",
    "        torch.save(ae.state_dict(), save_model_path)\n",
    "\n",
    "        print(\"\\nDone, trained model saved at\", save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
